{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b66723ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 2: JIT compilation and Physics Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b5060f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95343fba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We saw how JAX has a numpy-like user API for performing array-based computing.\n",
    "\n",
    "It has the capability to perform Composable Function Transformations on our Python/NumPy code and use it with the XLA compiler to perform super-fast computations on CPU, GPU or TPU\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\text{Python} \\rightarrow \\text{Intermediate Representation} \\rightarrow \\text{Transformations}$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e093b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7520ccbc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ lambda ; a:f32[100]. let\n",
      "    b:f32[100] = integer_pow[y=2] a\n",
      "    c:f32[100] = integer_pow[y=3] a\n",
      "    d:f32[100] = sub b c\n",
      "    e:f32[100] = sub d a\n",
      "    f:f32[] = reduce_sum[axes=(0,)] e\n",
      "  in (f,) }\n"
     ]
    }
   ],
   "source": [
    "# Let's try a simple example - we already saw this in the last lecture\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "from jax import make_jaxpr\n",
    "\n",
    "def fn(tuple_arr):\n",
    "    \n",
    "    sum_val = jnp.sum(tuple_arr**2 - tuple_arr**3 - tuple_arr)\n",
    "    \n",
    "    return sum_val\n",
    "\n",
    "\n",
    "print(make_jaxpr(fn)(jnp.ones(100)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42123a2d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffe62f8",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2886f154",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<div>\n",
    "<center>\n",
    "<img src=\"figures/Py_IR_Trans.png\" width=\"1000\"/>\n",
    "</center>\n",
    "</div>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3e8fc8",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815796d0",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b8781",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Examples of composable transformations include automatic differentiation, JIT compilation, parallelization on multi-core hardware (pmap), automatic vectorization (vmap) and automatic differentiation.\n",
    "\n",
    "We explored automatic differentiation in the last lecture.\n",
    "\n",
    "In this lecture, we will explore JIT compilation as well as find out a bit about how JAX works under the hood with these intermediate representations and transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d136def",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fa6cbd",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcae294",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### JIT Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bded9d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d08ccdf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "JIT stands for Just-In-Time compilation, as opposed to AOT (Ahead-Of-Time) compilation.\n",
    "\n",
    "As the name suggests, compilation of the code happens $\\textit{just in time}$ for computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dfeeca",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765ed537",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a98233",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "During JIT compilation, JAX applies a series of optimizations on primitive `lax` operations to generate efficient XLA executable code on CPU, GPU or TPU.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<div>\n",
    "<center>\n",
    "<img src=\"figures/JAX_workflow.png\" width=\"1000\"/>\n",
    "    </center>\n",
    "</div>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3775a287",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9386a7e2",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba334e50",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once a function has been JIT-compiled, JAX caches the resulting XLA code so that it can be re-used in subsequent calls. \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\\textbf{This is where the power of JIT compilation comes in} \\\\ \\textbf{ - after an initial compilation phase,} \\\\ \\textbf{the subsequent calls to the JIT-compiled function are super fast!}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892b219f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "11982fcc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.47 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "56.5 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from jax import jit\n",
    "\n",
    "def fn(tuple_arr):\n",
    "    \n",
    "    return jnp.sum(tuple_arr ** 2 - tuple_arr ** 3 - tuple_arr)\n",
    "\n",
    "%timeit -r1 -n1 fn(jnp.ones(100)).block_until_ready()\n",
    "\n",
    "fn_compiled = jit(fn)\n",
    "%timeit -r1 -n1 fn_compiled(jnp.ones(100)).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083b1f87",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429c011f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As mentioned before, the power of JIT compilation only comes in when we have intend to repeat function calls many times - for example finding the minimum of a loss function by repeatedly calling the loss function and it's gradient.\n",
    "\n",
    "During the first computation, JAX performs the optimizations which takes some time. - hence the slowed down computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4928970d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c5cc0ab1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.64 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "490 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Let's try again\n",
    "\n",
    "%timeit -r1 -n1 fn(jnp.ones(100)).block_until_ready()\n",
    "\n",
    "%timeit -r1 -n1 fn_compiled(jnp.ones(100)).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c898ba",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2721c2a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "42.9 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Let's try again with a different sized array\n",
    "\n",
    "%timeit -r1 -n1 fn(jnp.ones(1000)).block_until_ready()\n",
    "\n",
    "%timeit -r1 -n1 fn_compiled(jnp.ones(1000)).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b320b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bc7b34ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.45 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "560 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Let's re-do the same computation with the same array as before\n",
    "\n",
    "%timeit -r1 -n1 fn(jnp.ones(1000)).block_until_ready()\n",
    "\n",
    "%timeit -r1 -n1 fn_compiled(jnp.ones(1000)).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97056a8",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fae6b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To better understand what's going on, let's find out how JAX performs under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c562fa9",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c52740b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced<ShapedArray(float32[100])>with<DynamicJaxprTrace(level=0/1)>\n",
      "-100.0\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from jax import jit\n",
    "\n",
    "# Equivalent to doing jit(fn), we can optionally put a decorator that does the same thing\n",
    "\n",
    "@jit\n",
    "def fn(tuple_arr):\n",
    "    \n",
    "    print(tuple_arr)   # Let's print out the input we passed to the function\n",
    "    \n",
    "    return jnp.sum(tuple_arr ** 2 - tuple_arr ** 3 - tuple_arr)\n",
    "\n",
    "\n",
    "print(fn(jnp.ones(100)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef4408c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21651fb6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "JAX first takes in the shape of the input, and uses it to create an abstract object called `ShapedArray` that has a specific data type and size.\n",
    "\n",
    "But it has no value!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65724cb3",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ad664310",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced<ShapedArray(float32[100])>with<DynamicJaxprTrace(level=1/1)>\n",
      "{ lambda ; a:f32[100]. let\n",
      "    b:f32[] = xla_call[\n",
      "      call_jaxpr={ lambda ; c:f32[100]. let\n",
      "          d:f32[100] = integer_pow[y=2] c\n",
      "          e:f32[100] = integer_pow[y=3] c\n",
      "          f:f32[100] = sub d e\n",
      "          g:f32[100] = sub f c\n",
      "          h:f32[] = reduce_sum[axes=(0,)] g\n",
      "        in (h,) }\n",
      "      name=fn\n",
      "    ] a\n",
      "  in (b,) }\n"
     ]
    }
   ],
   "source": [
    "print(jax.make_jaxpr(fn)(jnp.ones(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe2332",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f13c01c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then, it passes this `ShapedArray` object through the primitive `lax` operations to create a computation chart for function execution. This part of the process is known as $\\textit{tracing}$.\n",
    "\n",
    "But note that the `ShapedArray` object has a specific size and type now `a:f32[100]` - but that is all it has. This object doesn't store any values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87e9eea",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb098f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once JAX finishes creating the computation chart with `lax` operations, it then optimizes the code for the XLA compiler to run on any machine of choice.\n",
    "\n",
    "This optimization step is then cached.\n",
    "\n",
    "Upon repeated executions with inputs of the same type and size, but any different value, the cached code is run on the machine with super-fast execution times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651fc1c4",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a2c76f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4fa509",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Key Summary of JIT compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04e347a",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4721ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By default JAX executes operations one at a time, in sequence.\n",
    "\n",
    "We saw an example of this in the last lecture with our purpose-ly bad written function and how JAX converted it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ac535",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0794d5f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[] b:f32[]. let\n",
       "    c:f32[] = div a b\n",
       "    d:f32[] = exp b\n",
       "    e:f32[] = sub c d\n",
       "    f:f32[] = div a b\n",
       "    g:f32[] = sin f\n",
       "    h:f32[] = div a b\n",
       "    i:f32[] = add g h\n",
       "    j:f32[] = exp b\n",
       "    k:f32[] = sub i j\n",
       "    l:f32[] = mul e k\n",
       "  in (l,) }"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see how JAX simplifies our expression with it's computation graph\n",
    "\n",
    "def fnc_jax(x1, x2):\n",
    "    \n",
    "    return (jnp.divide(x1,x2) - jnp.exp(x2))*(jnp.sin(jnp.divide(x1,x2)) + jnp.divide(x1,x2) - jnp.exp(x2))\n",
    "\n",
    "jax.make_jaxpr(fnc_jax)(1.0,1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b167cd",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3525ee2a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using a just-in-time (JIT) compilation decorator, sequences of operations can be optimized together and run at once.\n",
    "\n",
    "Let's see this in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a1e30c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7567f36b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.6\n",
      "HloModule jit_fnc_jax, entry_computation_layout={(f32[],f32[])->f32[]}, allow_spmd_sharding_propagation_to_output={true}\n",
      "\n",
      "%fused_computation (param_0.2: f32[], param_1.4: f32[]) -> f32[] {\n",
      "  %param_0.2 = f32[] parameter(0)\n",
      "  %param_1.4 = f32[] parameter(1)\n",
      "  %exponential.0 = f32[] exponential(f32[] %param_1.4), metadata={op_name=\"jit(fnc_jax)/jit(main)/exp\" source_file=\"/tmp/ipykernel_42962/377062428.py\" source_line=8}\n",
      "  %subtract.1 = f32[] subtract(f32[] %param_0.2, f32[] %exponential.0), metadata={op_name=\"jit(fnc_jax)/jit(main)/sub\" source_file=\"/tmp/ipykernel_42962/377062428.py\" source_line=8}\n",
      "  %sine.0 = f32[] sine(f32[] %param_0.2), metadata={op_name=\"jit(fnc_jax)/jit(main)/sin\" source_file=\"/tmp/ipykernel_42962/377062428.py\" source_line=8}\n",
      "  %add.0 = f32[] add(f32[] %sine.0, f32[] %param_0.2), metadata={op_name=\"jit(fnc_jax)/jit(main)/add\" source_file=\"/tmp/ipykernel_42962/377062428.py\" source_line=8}\n",
      "  %subtract.0 = f32[] subtract(f32[] %add.0, f32[] %exponential.0), metadata={op_name=\"jit(fnc_jax)/jit(main)/sub\" source_file=\"/tmp/ipykernel_42962/377062428.py\" source_line=8}\n",
      "  ROOT %multiply.0 = f32[] multiply(f32[] %subtract.1, f32[] %subtract.0), metadata={op_name=\"jit(fnc_jax)/jit(main)/mul\" source_file=\"/tmp/ipykernel_42962/377062428.py\" source_line=8}\n",
      "}\n",
      "\n",
      "ENTRY %main.13 (Arg_0.1: f32[], Arg_1.2: f32[]) -> f32[] {\n",
      "  %Arg_0.1 = f32[] parameter(0), sharding={replicated}\n",
      "  %Arg_1.2 = f32[] parameter(1), sharding={replicated}\n",
      "  %divide.3 = f32[] divide(f32[] %Arg_0.1, f32[] %Arg_1.2), metadata={op_name=\"jit(fnc_jax)/jit(main)/div\" source_file=\"/tmp/ipykernel_42962/377062428.py\" source_line=8}\n",
      "  ROOT %fusion = f32[] fusion(f32[] %divide.3, f32[] %Arg_1.2), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(fnc_jax)/jit(main)/mul\" source_file=\"/tmp/ipykernel_42962/377062428.py\" source_line=8}\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we perform an AOT compilation to show what goes on after compilation - yes JAX also supports AOT when necessary\n",
    "\n",
    "from jax import jit\n",
    "\n",
    "print(jax.__version__)\n",
    "\n",
    "print(jit(fnc_jax).lower(1., 1.).compile().as_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87171bb",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d81f35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Not all JAX code can be JIT compiled, as it requires array shapes to be static & known at compile time.\n",
    "\n",
    "As we saw before, JIT compilation traces a `ShapedArray` object with fixed shape and type - you cannot change it's shape after compilation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b268a220",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82e386be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NonConcreteBooleanIndexError",
     "evalue": "Array boolean indices must be concrete; got ShapedArray(bool[100])\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.NonConcreteBooleanIndexError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNonConcreteBooleanIndexError\u001b[0m              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39msum(sum_arr)       \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# We cannot have output with shapes not known explicitly at compile time\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfn_bad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m, in \u001b[0;36mfn_bad\u001b[0;34m(tuple_arr)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@jit\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn_bad\u001b[39m(tuple_arr):\n\u001b[1;32m      4\u001b[0m     sum_arr \u001b[38;5;241m=\u001b[39m tuple_arr \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m tuple_arr \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m-\u001b[39m tuple_arr\n\u001b[0;32m----> 6\u001b[0m     sum_arr \u001b[38;5;241m=\u001b[39m \u001b[43msum_arr\u001b[49m\u001b[43m[\u001b[49m\u001b[43msum_arr\u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m]\u001b[49m          \u001b[38;5;66;03m# Remove negative values from array\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39msum(sum_arr)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:3942\u001b[0m, in \u001b[0;36m_rewriting_take\u001b[0;34m(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[1;32m   3936\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(aval, core\u001b[38;5;241m.\u001b[39mDShapedArray) \u001b[38;5;129;01mand\u001b[39;00m aval\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m () \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   3937\u001b[0m         dtypes\u001b[38;5;241m.\u001b[39missubdtype(aval\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   3938\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39missubdtype(aval\u001b[38;5;241m.\u001b[39mdtype, dtypes\u001b[38;5;241m.\u001b[39mbool_) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   3939\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(arr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mint\u001b[39m)):\n\u001b[1;32m   3940\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m lax\u001b[38;5;241m.\u001b[39mdynamic_index_in_dim(arr, idx, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 3942\u001b[0m treedef, static_idx, dynamic_idx \u001b[38;5;241m=\u001b[39m \u001b[43m_split_index_for_jit\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3943\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n\u001b[1;32m   3944\u001b[0m                unique_indices, mode, fill_value)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:4021\u001b[0m, in \u001b[0;36m_split_index_for_jit\u001b[0;34m(idx, shape)\u001b[0m\n\u001b[1;32m   4017\u001b[0m idx \u001b[38;5;241m=\u001b[39m _eliminate_deprecated_list_indexing(idx)\n\u001b[1;32m   4019\u001b[0m \u001b[38;5;66;03m# Expand any (concrete) boolean indices. We can then use advanced integer\u001b[39;00m\n\u001b[1;32m   4020\u001b[0m \u001b[38;5;66;03m# indexing logic to handle them.\u001b[39;00m\n\u001b[0;32m-> 4021\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[43m_expand_bool_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4023\u001b[0m leaves, treedef \u001b[38;5;241m=\u001b[39m tree_flatten(idx)\n\u001b[1;32m   4024\u001b[0m dynamic \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(leaves)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:4334\u001b[0m, in \u001b[0;36m_expand_bool_indices\u001b[0;34m(idx, shape)\u001b[0m\n\u001b[1;32m   4330\u001b[0m   abstract_i \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mget_aval(i)\n\u001b[1;32m   4332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(abstract_i) \u001b[38;5;129;01mis\u001b[39;00m ConcreteArray:\n\u001b[1;32m   4333\u001b[0m   \u001b[38;5;66;03m# TODO(mattjj): improve this error by tracking _why_ the indices are not concrete\u001b[39;00m\n\u001b[0;32m-> 4334\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNonConcreteBooleanIndexError(abstract_i)\n\u001b[1;32m   4335\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _ndim(i) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   4336\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAX arrays do not support boolean scalar indices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNonConcreteBooleanIndexError\u001b[0m: Array boolean indices must be concrete; got ShapedArray(bool[100])\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.NonConcreteBooleanIndexError"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def fn_bad(tuple_arr):\n",
    "    \n",
    "    sum_arr = tuple_arr ** 2 - tuple_arr ** 3 - tuple_arr\n",
    "    \n",
    "    sum_arr = sum_arr[sum_arr>=0.0]          # Remove negative values from array\n",
    "    \n",
    "    return jnp.sum(sum_arr)       \n",
    "\n",
    "\n",
    "# We cannot have output with shapes not known explicitly at compile time\n",
    "\n",
    "print(fn_bad(jnp.ones(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ca5e7d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de34d7e7",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367e56d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Things to Keep in Mind when using JAX and JIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a133d4b8",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a65e1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "JAX provides the `jax.numpy` wrapper to mimic the more familiar interface for users.\n",
    "\n",
    "Under the hood, howoever, JAX performs it's computations using the more powerful, but stricter, `jax.lax` API. \n",
    "\n",
    "We just explored this with some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd215f88",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0813dd23",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f68e2c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But `jax.numpy` cannot always be directly used as a replacement to `numpy`.\n",
    "\n",
    "For example, unlike NumPy arrays, JAX arrays are always immutable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd95584",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "55819861",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jaxlib.xla_extension.DeviceArray'>\n",
      "<class 'numpy.ndarray'>\n",
      "[1 0 3 4]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<class 'jaxlib.xla_extension.DeviceArray'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1578/1071326328.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0marr_jnp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_102b_swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_unimplemented_setitem\u001b[0;34m(self, i, x)\u001b[0m\n\u001b[1;32m   6793\u001b[0m          \u001b[0;34m\"or another .at[] method: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6794\u001b[0m          \"https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\")\n\u001b[0;32m-> 6795\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6797\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_operator_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndigits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<class 'jaxlib.xla_extension.DeviceArray'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "arr_jnp = jnp.array([1,2,3,4])\n",
    "\n",
    "print(type(arr_jnp))\n",
    "\n",
    "arr_np = np.array([1,2,3,4])\n",
    "\n",
    "print(type(arr_np))\n",
    "\n",
    "arr_np[1] = 0\n",
    "print(arr_np)\n",
    "\n",
    "arr_jnp[1] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7194bb5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb26b4c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "45de0293",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 3 4]\n"
     ]
    }
   ],
   "source": [
    "# For updating individual elements, JAX provides an indexed update syntax that returns an updated copy:\n",
    "\n",
    "arr_jnp = arr_jnp.at[1].set(0)  # Re-assign the mutated array\n",
    "print(arr_jnp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6162bf1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefad93e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d7abf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pure Functions\n",
    "\n",
    "The proper way to use JAX and JIT compilation is to use it only on functionally pure Python functions.\n",
    "\n",
    "What does this mean? A pure function will always return the same result if invoked with the same inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58448ba",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40ac2df1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced<ShapedArray(float32[100])>with<DynamicJaxprTrace(level=0/2)>\n",
      "First call:  -100.0\n",
      "Second call:  -100.0\n"
     ]
    }
   ],
   "source": [
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def fn(tuple_arr):\n",
    "    \n",
    "    print(tuple_arr)   # This is a side-effect\n",
    "    \n",
    "    return jnp.sum(tuple_arr ** 2 - tuple_arr ** 3 - tuple_arr)\n",
    "\n",
    "\n",
    "# The side-effects appear during the first run\n",
    "print (\"First call: \", jit(fn)(jnp.ones(100)))\n",
    "\n",
    "# Subsequent runs with parameters of same type and shape may not show the side-effect\n",
    "# This is because JAX now invokes a cached compilation of the function\n",
    "print (\"Second call: \", jit(fn)(jnp.ones(100)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9447cfbe",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5da0b8a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call:  -95.0\n",
      "Second call:  -95.0\n",
      "Third call, different type:  -85.0\n"
     ]
    }
   ],
   "source": [
    "# Global variables defined outside the function will be stored as static when JIT caches the computation\n",
    "\n",
    "bias = 5.0\n",
    "\n",
    "def fn(tuple_arr):\n",
    "    \n",
    "    return jnp.sum(tuple_arr ** 2 - tuple_arr ** 3 - tuple_arr)+bias\n",
    "\n",
    "# JAX captures the value of the global during the first run\n",
    "print (\"First call: \", jit(fn)(jnp.ones(100)))\n",
    "\n",
    "bias = bias + 10.0  # Update the global\n",
    "\n",
    "# Subsequent runs may silently use the cached value of the globals\n",
    "print (\"Second call: \", jit(fn)(jnp.ones(100)))\n",
    "\n",
    "# JAX re-runs the Python function when the type or shape of the argument changes\n",
    "# This will end up reading the latest value of the global\n",
    "print (\"Third call, different type: \", jit(fn)(jnp.ones(100, dtype='int32')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dbf00e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4385a690",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\textbf{General rule of thumb:}$ \n",
    "\n",
    "$$\\textbf{All the input data should be passed through the function parameters,} \\\\ \n",
    "\\textbf{all the results should be output through the function results.}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "This will also help with the constant-folding issue for large constants passed to the function.\n",
    "\n",
    "For more $\\textit{sharp edges}$ to be careful of, read https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c8c27",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71416c69",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae4ec4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Physics Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc2903e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d828d971",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As an example, let us continue with the $H \\rightarrow WW \\rightarrow 2\\ell2\\nu$ example we explored in the ML introduction lectures.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<div>\n",
    "<center>\n",
    "<img src=\"figures/WW_Feynman.png\" width=\"750\"/>\n",
    "    </center>\n",
    "</div>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338c2429",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a75bba8",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5c30d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img align=\"right\" src=\"figures/Discriminant_Prelim.png\" width=\"500\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "We trained an ML discriminant between the Signal process $H \\rightarrow WW \\rightarrow 2\\ell2\\nu$ and the various backgrounds.\n",
    "\n",
    "<br>\n",
    "\n",
    "We will use this binned histogram to do a statistical inference on the parameter $\\mu$, where $\\mu$ is characterized as the signal-strength: $N_\\text{tot} = \\mu S + B$.\n",
    "\n",
    "<br>\n",
    "\n",
    "Using the discriminant, we can calculate the full likelihood for the value of POI $\\mu$.\n",
    "\n",
    "<br>\n",
    "\n",
    "We approximate the likelihood of observing $n_i$ events in bin $i$ of the discriminant using a Poisson likelihood centered around the MC expected value in that bin. \n",
    "\n",
    "<br>\n",
    "\n",
    "This gives us an expression for the full likelihood, inclusive of all bins:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\mathcal{L} (\\mu | \\mathcal{D}_{obs}) = \\prod_{i}^\\text{bins} \\frac{e^{-\\nu (\\mu)} \\nu^{N_{obs}} (\\mu)}{N_{obs} !}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c40307",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaa9d9a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b266f78b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Systematic uncertainties arise in all physics analyses where there exist uncertainties in the predictions of any of the simulation steps. \n",
    "\n",
    "Uncertainties that arise in the detector simulation are normally categorised as experimental uncertainties. Uncertainty in any of the other processing steps is usually categorised as a theoretical uncertainty.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<div>\n",
    "<center>\n",
    "<img src=\"figures/Unc_chart.png\" width=\"750\"/>\n",
    "    </center>\n",
    "</div>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Theoretical and experimental uncertainties are grouped together as $\\textit{systematic uncertainties}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b524a0",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10977dab",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a342aac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The effect of systematic uncertainties is described in the probability model using so-called constrained NPs, denoted here by $\\theta$:\n",
    "\n",
    "$$\\mathcal{L}_\\text{phys} (\\mu, \\theta | \\mathcal{D}_{obs}) = \\prod_{i}^\\text{bins} \\frac{e^{-\\nu (\\mu, \\theta)} \\nu (\\mu, \\theta)^{N_{obs}} }{N_{obs} !}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "The detailed probability model for the auxiliary measurement is approximated as a Gaussian, with central values and errors estimated using either experimental data or theoretical assumptions. \n",
    "\n",
    "The constraint term arising from the subsidiary measurement in the likelihood is written simply as (after some convenient normalizations):\n",
    "\n",
    "$$\\mathcal{L}_\\text{constraint} (\\theta) = \\prod_{i}^{N_\\text{syst}} \\frac{1}{2\\pi} \\exp \\left(-\\frac{\\theta^2}{2}\\right)$$\n",
    "\n",
    "The full $\\theta$-parametrized likelihood is then:\n",
    "\n",
    "\n",
    "$$\\mathcal{L} (\\mu, \\theta | \\mathcal{D}_{obs}) = \\mathcal{L}_\\text{phys} (\\mu, \\theta | \\mathcal{D}_{obs}) \\cdot \\mathcal{L}_\\text{constraint} (\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31a186d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf52428",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a89b00",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In order to do a hypothesis test using the likelihoods, we must first define a test statistic $t_\\mu$. \n",
    "\n",
    "In a typical physics analysis, a profiled log of the likelihood ratio between the hypothesis of interest and a nominal hypothesis is chosen as the test statistic. \n",
    "\n",
    "<br>\n",
    "\n",
    "$$t_\\mu = -2 \\log \\frac{\\mathcal{L} (\\mu, \\hat{\\hat{\\theta}} | \\mathcal{D}_{obs})}{\\mathcal{L} (\\hat{\\mu}, \\hat{\\theta} | \\mathcal{D}_{obs})}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "where $\\hat{\\mu}$, $\\hat{\\theta}$ are the global best-fit values of the parameters $\\mu$ and $\\theta$ that maxmizes the likelihood:\n",
    "\n",
    "$$\\hat{\\mu}, \\hat{\\theta} = \\underset{\\mu, \\theta}{\\operatorname{argmax}}\\mathcal{L} (\\mu, \\theta | \\mathcal{D}_{obs})$$\n",
    "\n",
    "<br>\n",
    "\n",
    "and $\\hat{\\hat{\\theta}}$ is the local best-fit value of $\\theta$ for a fixed value of parameter $\\mu$ being scanned:\n",
    "\n",
    "$$\\hat{\\hat{\\theta}} = \\underset{\\theta}{\\operatorname{argmax}}\\mathcal{L} (\\mu_\\text{fix}, \\theta | \\mathcal{D}_{obs})$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The best-fit value of the parameter of interest (POI) $\\mu$ given an observed dataset can be calculated by minimizing the log likelihood test statistic:\n",
    "\n",
    "$$\\mu_\\text{MLE} = \\underset{\\mu}{\\operatorname{argmin}} t_\\mu $$\n",
    "\n",
    "<br>\n",
    "\n",
    "This quantity $\\mu_\\text{MLE}$ is called Maximum Likelihood Estimator (MLE) of the POI $\\mu$.\n",
    "\n",
    "And of course, this is yet another problem that can be solved using gradient-based optimizations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f0ac6e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f35f08",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c908e35a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There is a very popular tool used in HEP traditionally, that has the capability to perform this optimization and find the best-fit values using Numerical Differentiation techniques!\n",
    "\n",
    "This tool is called Minuit (https://en.wikipedia.org/wiki/MINUIT)\n",
    "\n",
    "For our purpose, we will be working with it's Python-friendly implementation iMinuit (https://iminuit.readthedocs.io/en/stable/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82ca8c5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706dfe49",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edd6189",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While the tool uses numerical differentiation by default, we can take advantage of the accelerated array-based computing capabilities of JAX.\n",
    "\n",
    "In order to do the fitting optimization, iMinuit makes several (often thousands in a full analysis) of repeated calls of the test statistic function until it finds the best fit value.\n",
    "\n",
    "This is where we can take advantage of JIT compilation for an accelerated computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0769058",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ee6ed9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We shall explore this and more in the today's exercise."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
