{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc0a8622",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lesson 1: Auto-Differentiation and JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840502ec",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc6e78f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdb51bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hi, I am Jay Sandesara. \n",
    "\n",
    "I will be giving a very gentle introduction to the use of auto-differentiation techniques using the JAX library. \n",
    "\n",
    "The lectures are focused for use in statistical data analysis at HEP experiments like the ATLAS and CMS.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb67d83f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54486956",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1756c568",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "I am a final year graduate student at the University of Massachusetts Amherst. \n",
    "\n",
    "I especially welcome questions and discussions on any of the topics. It will help both you and me gain a deeper understanding of the subject!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a9feb0",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b13ff30",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4757a516",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's begin!\n",
    "\n",
    "I dont need to tell you that differentiation is used everywhere in physics! Everywhere from classical Newtonian and Lagrangian mechanics to the Schrodinger equations and the theories of relativity - we need to compute derivatives to do physics.\n",
    "\n",
    "\n",
    "<div>   \n",
    "    <center>\n",
    "        <img src=\"figures/Theory_applications.png\" width=\"1000\"/>\n",
    "    </center>\n",
    "    </div>\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaef3f59",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e596c68a",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d1f468",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But that's not all! \n",
    "\n",
    "Differentiation is also very imporant for use in statistical tool-sets used to make physics inference in experimental high energy physics.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div>   \n",
    "<center>\n",
    "        <img src=\"figures/HEP_applications.png\" width=\"1000\"/>\n",
    "</center>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "This latter application will be the focus of this set of lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5827e4c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318504c6",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a5fec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us start with the most popular statistical tool nowadays - Machine Learning. \n",
    "\n",
    "In ML, we typically use differentiation to perform gradient-based optimization of the ML model parameters. For example, we use gradient descent to minimize a loss function $\\mathcal{L}(\\omega)$. \n",
    "\n",
    "Here the $\\omega$ refer to the set of tune-able parameters in a ML model, such as the neurons in a Neural Network (NN). Our task is then to find the optimal set of parameter values $\\omega$ that minimizes the loss function:\n",
    "\n",
    "$$\\underset{\\omega}{\\operatorname{argmin}} \\mathcal{L}(\\omega)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e0a026",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0088a85a",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1dcf51",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How do we do this? A commonly used algorithm, called the gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. \n",
    "\n",
    "The idea is to take repeated steps in the opposite direction of the gradient of the function at the current point, because this is the direction of steepest descent.\n",
    "\n",
    "<div>\n",
    "<center>\n",
    "<img src=\"figures/func_min.png\" width=\"500\"/>\n",
    "</center>\n",
    "</div>\n",
    "\n",
    "$$\\vec{\\omega}_{i+1} = \\vec{\\omega}_{i} - \\lambda \\nabla_\\omega \\mathcal{L}(\\vec{\\omega}_i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2f9745",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff429cd",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eaf963",
   "metadata": {},
   "source": [
    "The figure is for a simple example with only one input parameter. \n",
    "\n",
    "The number of parameters in a typical NN can range from thousands to billions. \n",
    "\n",
    "Time efficient and accurate derivative estimation is thus a very important problem in computing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4104b3",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee18d661",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262530d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do we calculate the derivatives using computers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4ccf6d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01543b03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's start with the human way of doing it. Let's say we have a function\n",
    "\n",
    "$$f(x) = x^2+5$$\n",
    "\n",
    "In Python this can be written as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06f62349",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \n",
    "    return x**2 + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7451eb7c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9b543e",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc55de8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We want to calculate the derivative. We simply define another function, that is a derivative of this simple function:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239bacb9",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a63099",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27ede66a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def f_prime(x):\n",
    "    \n",
    "    return 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c75d7f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9550fd94",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8b38c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is easy, we now have a new Python function that gives us the derivative of $f(x)$ for any value of $x$! \n",
    "\n",
    "This way of calculating the derivatives using a computer is known as $\\textbf{Manual Differentiation}$, for obvious reasons.\n",
    "\n",
    "This is commonly used to compute derivatives in theory computations.\n",
    "\n",
    "While this type of approach is fairly intuitive, it can quickly become tedious for more complicated functions - common in statistics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65936b2",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22680c6f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb73bf61",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another way we can compute the derivatives is using $\\textbf{Numerical Differentiation}$, which uses finite differences to compute the derivatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdd43d6",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a496cdc",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820e6d00",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A derivative of a scalar valued function $f(x)$ for an n-dimensional vector $x$ is defined as follows:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} = \\underset{h \\rightarrow 0}{\\operatorname{lim}} \\frac{f(x+h)-f(x)}{h} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28496226",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2940cab6",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773d3e59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can update this definition so as to include very small, but non-zero values of h:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} \\approx \\frac{f(x+h)-f(x)}{h} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79630515",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \n",
    "    return x**2 + 5\n",
    "\n",
    "def f_prime_man(x):\n",
    "    \n",
    "    return 2*x\n",
    "\n",
    "def f_prime_num(x, h):\n",
    "    \n",
    "    return ((f(x+h)) - f(x))/h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac694088",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770d8f47",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bf7f375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.00001000001393\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "print(f_prime_num(1.0,1e-5))\n",
    "print(f_prime_man(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4249f7a",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c883fd35",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e2c522c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'step size')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Grid of step sizes\n",
    "scan_h_values = np.linspace(1e-12,1e-4,num=30)\n",
    "\n",
    "# Evaluate the function at 1.0\n",
    "ones = np.ones_like(scan_values)\n",
    "\n",
    "# Error = Manual Diff (Exact) - Numerical Diff (Approx)\n",
    "error_estimates = f_prime_man(ones) - f_prime_num(ones, scan_h_values) \n",
    "        \n",
    "plt.plot(scan_h_values, error_estimates)\n",
    "plt.axhline(y = 0.0, color = 'g', linestyle = '--')\n",
    "plt.ylabel('Errors')\n",
    "plt.xlabel('step size')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb7c1a5",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdd814a",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbbc3c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An obvious limitation from this approach is the choice of the step size h. \n",
    "\n",
    "One needs to balance between the truncation error and the rounding error resulting from the limited precision of the function itself.\n",
    "\n",
    "But as we saw, these errors can be made minimal for practical statistical purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ba8db3",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e4af20",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77007f8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The main limitation of this approach comes from elsewhere. Generalizing the derivative for more than one parameter:\n",
    "    \n",
    "$$\\frac{\\partial f}{\\partial x_i} \\approx \\frac{f(x_i+h e_i)-f(x_i)}{h} $$\n",
    "\n",
    "We can see how such a derivative requires $n$ individual computations. For ML models, the number of tune-able parameters typically range from thousands to billions, making this method un-feasible in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcd42d2",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1474ab6d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6f54aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\textbf{Symbolic Differentiation}$ program can convert a function of our choice into it's respective derivative function, by transforming the function using the standard derivative tools. \n",
    "\n",
    "This is essentially an automated version of the Manual Differentiation technique. \n",
    "\n",
    "<br>\n",
    "\n",
    "<div>\n",
    "<center>\n",
    "<img src=\"figures/Sym_Diff.png\" width=\"500\"/>\n",
    "</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271bcffb",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c462ac20",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b88fdb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This might seem like an optimal technique but it suffers from major flaws. One such limitation is known as Expression Swelling. \n",
    "\n",
    "What this means is that a function's derivative can turn into a very convoluted expression when written out explicitly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d1e745",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56c43c7",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224a5a1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's take an example of the soft ReLU function, commonly used as an activation function in NN models:\n",
    "\n",
    "$$ \\log \\left(1.0 + e^{\\vec{w}\\cdot \\vec{x} + \\vec{b}} \\right) $$\n",
    "\n",
    "The derivative of this, w.r.t only two parameters is fairly involved:\n",
    "\n",
    "$$ \\frac{e^{b_1+b_2+ w_1 x + w_2 \\log \\left[ 1 + e^{b_1+w_1 x} \\right] } w_2 x}{(1 + e^{b_1+w_1 x})\\left(1 + e^{b_2+w_2 \\log \\left[1 + e^{b_1+w_1 x}\\right]}\\right)}$$\n",
    "\n",
    "This is with just two parameters; with thousands of them (as is often typical in a hidden layer in an ML model) this quickly becomes so complex, it is impractical to calculate efficiently. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b265ec23",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0979995e",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f560bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Moreover, the function passed to a symbolic differentiation program needs to be of a closed form - i.e. we cannot have loops or if/else conditions in the function. \n",
    "\n",
    "As we will see in the next lectures, control flow operations are particularly essential in the calculation of Hessian in an experimental analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88588699",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48685704",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081a79f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It might seem hopeless than to compute these second derivatives with millions of parameters in a time practical way - and yet we keep seeing NNs with billions of tune-able parameters! \n",
    "\n",
    "Enter $\\textbf{Automatic Differentiation}$. Rather than produce an expression for the derivative of a function, it calculates directly the exact numerical value of the function and it's derivatives for a given set of input parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d520b52",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f75bc0d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd95d487",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To best understand automatic differentiation, it is easiest to start with a simple example. Let's say we want to find the value of the first-order derivative of the following function:\n",
    "\n",
    "$$f(x_1, x_2) = \\left(\\frac{x_1}{x_2} - e^{x_2} \\right) \\cdot \\left(sin\\left(\\frac{x_1}{x_2}\\right) + \\frac{x_1}{x_2} - e^{x_2}\\right)$$\n",
    "\n",
    "Seems fairly complex, especially if we try to perform manual or symbolic differentiation on this. But with some cleverness we can convert this function into a series of elementary computations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8af10b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755fb79c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb961548",
   "metadata": {},
   "source": [
    "Let's write the function using Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25a3dfb",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70ad1d6",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bca9b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the above function as a combination of it's elementary operations\n",
    "\n",
    "def fn(x1, x2):\n",
    "    \n",
    "    a = x1/x2      \n",
    "    \n",
    "    b = np.exp(x2)     \n",
    "    \n",
    "    c = np.sin(a)\n",
    "    \n",
    "    d = a - b\n",
    "    \n",
    "    e = c + d\n",
    "    \n",
    "    g = d * e\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fdf76a",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be416dc3",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58882552",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Do you see what we did here? Let's plot it out:\n",
    "\n",
    "<div>\n",
    "<center>\n",
    "<img src=\"figures/forward_pass_nodiff.png\" width=\"1000\"/>\n",
    "    </center>\n",
    "</div>\n",
    "\n",
    "We start with the most basic operations involved in the function and then we use these computations to eventually compute the full function value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2723696",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be336ea",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5104697b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This might seem like a simple trick - but together with the chain rule of differentiation, this can be exploited to compute accurate and efficient derivatives of arbitrarily complex functions.\n",
    "\n",
    "Let's continue with the function above. Let's re-write it as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb18453",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673d74ae",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bc4e006",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Write the derivative of the function using chain rule\n",
    "\n",
    "def fn_prime(x1, x2):\n",
    "    \n",
    "    a = x1/x2      \n",
    "    \n",
    "    a_prime_x1 = 1/x2                         # Explicitly compute derivative expressions wrt x1 parameter\n",
    "    \n",
    "    b = jnp.exp(x2) \n",
    "    \n",
    "    b_prime_x1 = 0\n",
    "    \n",
    "    c = jnp.sin(a)\n",
    "    \n",
    "    c_prime_x1 = np.cos(a)*a_prime_x1          # Note how we start using the values of previous computations\n",
    "\n",
    "    d = a - b\n",
    "    \n",
    "    d_prime_x1 = a_prime_x1 - b_prime_x1\n",
    "    \n",
    "    e = c + d\n",
    "    \n",
    "    e_prime_x1 = c_prime_x1 + d_prime_x1 \n",
    "     \n",
    "    f = d * e\n",
    "    \n",
    "    h_prime_x1 = ( d_prime_x1 * e ) + ( d * e_prime_x1 )\n",
    "    \n",
    "    \n",
    "    return h_prime_x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc3b5d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df6a35b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1a34f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An auto-differentiation algorithm is in essence doing something very similar! Let's plot it out again:\n",
    "\n",
    "<div>\n",
    "    <center>\n",
    "<img src=\"figures/forward_pass_diff.png\" width=\"1000\"/>\n",
    "    </center>\n",
    "</div>\n",
    "\n",
    "During a so-called forward pass, the auto-diff algorithm calculates and stores the values of each elementary operation and their respective derivatives just like in the function ```fn_prime(x1, x2)``` described in the last slide for example. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5279a48",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9437817f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2af05d",
   "metadata": {},
   "source": [
    "One can then easily calculate the value of the derivative wrt ```x1``` or ```x2``` at the specified input value, just as in ```fn_prime(x1, x2)``` defined in the previous slide. (For derivative with ```x2``` we need to do another forward pass)\n",
    "\n",
    "This is referred to as the forward mode auto-diff to be specific, as opposed to reverse mode auto-diff, but we will get to this distinction in the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8874ed0b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f98acf7",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce882cee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note how we only compute the derivatives of a few basic expressions and then use the chain rule to compute the full derivative using these intermediate values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eadfe9",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d7f2cd",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb89fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We no longer need a blown-up expression for a derivative that does repreated calculations of the same kind - we recycle the values we already calculated once. \n",
    "\n",
    "And we no longer need to worry about truncation errors - the computed derivatives are exact!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f335939",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0945f369",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999daa4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We thus bypass the limitations of both Manual and Symbolic differentiations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e2f2d8",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33b7f96",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4f5bf6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These are some of the fundamentals of automatic differentiation, to help you get started. \n",
    "\n",
    "You will see more in today's exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f2eb8",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d04b5c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd004d4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is JAX?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4e031c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc6a079",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1ed175",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "JAX has a numpy-like user API for performing array-based computing.\n",
    "\n",
    "It has the capability to perform something called Composable Function Transformations - transforming our Python and NumPy code to perform automatic differentiation, JIT compilation, parallelization on multi-core hardware, etc.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\text{Python} \\rightarrow \\text{Intermediate Representation} \\rightarrow \\text{Transformations}$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22af79fe",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c1600d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As such, JAX is not just a tool for auto-differentiation. It also provides a range of tools for accelerated computation. \n",
    "\n",
    "It is built on top of the XLA (Accelerated Linear Algebra) compiler, which allows it to run fast on a variety of hardware platforms, including CPUs, GPUs, and TPUs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaa95e2",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49b67fb",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e1279a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One of the key features of JAX includes JIT compilation, which can speed up a piece of code by compiling it just-in-time. \n",
    "\n",
    "This can save a lot of time and effort, especially for complex functions with many parameters. \n",
    "\n",
    "We will be exploring more on the workings of JAX and of JIT compilation in the next two lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f348f95",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5d9571",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2c3197",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For now, let's explore the automatic differentiation capabilities of JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577df2e9",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3114e6",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eaa975",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We saw how forward auto-differntiation works in practice using the functions ```fn``` and ```fn_prime```. Let us see how JAX handles this function and computes it's derivative.\n",
    "\n",
    "First let us start with the simpler example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eef5fe",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c9618f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a98eedd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp\n",
    "\n",
    "def f(x):\n",
    "    \n",
    "    return jnp.power(x,2)+5       #Note how we replaced the regular numpy with JAX's numpy API\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551a300b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6522ff22",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634180fc",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "As mentioned before, JAX creates intermediate representation of our Python function before execution.\n",
    "\n",
    "It does so, using a fixed set of $\\textit{primitive}$ operations handled by `jax.lax`, e.g. `lax.add`,`lax.sub`, etc. We will explore more of this $\\textit{tracing}$ step in the next lecture.\n",
    "\n",
    "These are exactly the type of elementary operations we used to transform our Python function before!\n",
    "\n",
    "JAX knows how to perform composable transformations, like `grad`, `jit`, etc on these primitive operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfb14bb",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225ead76",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74eea7c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[]. let\n",
       "    b:f32[] = integer_pow[y=2] a\n",
       "    c:f32[] = add b 5.0\n",
       "  in (c,) }"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us check this out. make_jaxpr shows us the intermediate representation performed by JAX\n",
    "\n",
    "jax.make_jaxpr(f)(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1572953",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c582523",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e9cd5fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[]. let\n",
       "    b:i32[1,1] = iota[dimension=0 dtype=int32 shape=(1, 1)] \n",
       "    c:i32[1,1] = add b 0\n",
       "    d:i32[1,1] = iota[dimension=1 dtype=int32 shape=(1, 1)] \n",
       "    e:bool[1,1] = eq c d\n",
       "    f:f32[1,1] = convert_element_type[new_dtype=float32 weak_type=False] e\n",
       "    g:f32[1,1] = slice[limit_indices=(1, 1) start_indices=(0, 0) strides=None] f\n",
       "    h:f32[1] = reshape[dimensions=None new_sizes=(1,)] g\n",
       "    i:f32[1] = convert_element_type[new_dtype=float32 weak_type=True] h\n",
       "    j:f32[] = integer_pow[y=2] a\n",
       "    k:f32[] = integer_pow[y=1] a\n",
       "    l:f32[] = mul 2.0 k\n",
       "    m:f32[1] = mul i l\n",
       "    _:f32[] = add j 5.0\n",
       "    n:f32[1] = slice[limit_indices=(1,) start_indices=(0,) strides=None] m\n",
       "    o:f32[] = reshape[dimensions=None new_sizes=()] n\n",
       "  in (o,) }"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let us check out how it performs the gradient under the hood after doing the intermediate representation\n",
    "\n",
    "#Does the output look familiar?\n",
    "\n",
    "jax.make_jaxpr(jax.jacfwd(f))(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abee878d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf48aba",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167bb7a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Okay, that was intuitive and just what we expected! \n",
    "\n",
    "Now let's do this again with the more complicated function that we computed by manual auto-diff earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5806b3ea",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d38978c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "611aceb3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[] b:f32[]. let\n",
       "    c:f32[] = div a b\n",
       "    d:f32[] = exp b\n",
       "    e:f32[] = sub c d\n",
       "    f:f32[] = div a b\n",
       "    g:f32[] = sin f\n",
       "    h:f32[] = div a b\n",
       "    i:f32[] = add g h\n",
       "    j:f32[] = exp b\n",
       "    k:f32[] = sub i j\n",
       "    l:f32[] = mul e k\n",
       "  in (l,) }"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see how JAX simplifies our expression with it's computation graph\n",
    "\n",
    "def fnc_jax(x1, x2):\n",
    "    \n",
    "    return (jnp.divide(x1,x2) - jnp.exp(x2))*(jnp.sin(jnp.divide(x1,x2)) + jnp.divide(x1,x2) - jnp.exp(x2))\n",
    "\n",
    "jax.make_jaxpr(fnc_jax)(1.0,1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3740e9e5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876f5aff",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14331054",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The `div a b` seems to be repeated three times in the JAX computation.\n",
    "\n",
    "This is because, as of now, we do not compile it using JIT. So the computation chart follows the way $\\textit{you}$ write the function.\n",
    "\n",
    "Once we do, JIT recognizes and de-duplicates the repeated operations where necessary. We shall explore this in the next lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b625a0",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cfeadc",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e424b06d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[] b:f32[]. let\n",
       "    c:f32[] = div a b\n",
       "    d:f32[] = exp b\n",
       "    e:f32[] = sub c d\n",
       "    f:f32[] = sin c\n",
       "    g:f32[] = add f c\n",
       "    h:f32[] = sub g d\n",
       "    i:f32[] = mul e h\n",
       "  in (i,) }"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's manually re-write the function to avoid repeated computation for simplicity\n",
    "\n",
    "def fnc_jax(x1, x2):\n",
    "    a = jnp.divide(x1,x2)\n",
    "    b = jnp.exp(x2)\n",
    "    return (a - b)*(jnp.sin(a) + a - b)\n",
    "\n",
    "jax.make_jaxpr(fnc_jax)(1.0,1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed4d5a7",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5f2b4e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb9fbec7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Gradient Result\n",
      "-3.523484\n",
      "\n",
      "\n",
      "JAX Gradient Result\n",
      "-3.523484\n"
     ]
    }
   ],
   "source": [
    "# Let us check if the derivative function we wrote earlier has the same result as JAX's autodiff calculation\n",
    "\n",
    "print(\"Manual Gradient Result\")\n",
    "print(fn_prime(1.0,1.0))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"JAX Gradient Result\")\n",
    "print(jax.grad(fnc_jax)(1.0,1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717fe0cc",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1f8bad",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a7d649be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[] b:f32[]. let\n",
       "    c:i32[1,1] = iota[dimension=0 dtype=int32 shape=(1, 1)] \n",
       "    d:i32[1,1] = add c 0\n",
       "    e:i32[1,1] = iota[dimension=1 dtype=int32 shape=(1, 1)] \n",
       "    f:bool[1,1] = eq d e\n",
       "    g:f32[1,1] = convert_element_type[new_dtype=float32 weak_type=False] f\n",
       "    h:f32[1,1] = slice[limit_indices=(1, 1) start_indices=(0, 0) strides=None] g\n",
       "    i:f32[1] = reshape[dimensions=None new_sizes=(1,)] h\n",
       "    j:f32[1] = convert_element_type[new_dtype=float32 weak_type=True] i\n",
       "    k:f32[] = div a b\n",
       "    l:f32[1] = div j b\n",
       "    m:f32[] = exp b\n",
       "    n:f32[] = sub k m\n",
       "    o:f32[] = sin k\n",
       "    p:f32[] = cos k\n",
       "    q:f32[1] = mul l p\n",
       "    r:f32[] = add o k\n",
       "    s:f32[1] = add q l\n",
       "    t:f32[] = sub r m\n",
       "    _:f32[] = mul n t\n",
       "    u:f32[1] = mul l t\n",
       "    v:f32[1] = mul n s\n",
       "    w:f32[1] = add_any u v\n",
       "    x:f32[1] = slice[limit_indices=(1,) start_indices=(0,) strides=None] w\n",
       "    y:f32[] = reshape[dimensions=None new_sizes=()] x\n",
       "  in (y,) }"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's check if our breakdown of the gradient function matches JAX's computation chart\n",
    "\n",
    "jax.make_jaxpr(jax.jacfwd(fnc_jax))(1.0,1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7b4e97",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d68e10a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41198dc0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You may compare this to our manual implementation `fn_prime(x1, x2)` of forward mode auto differentiation and see the similarities!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265eeb1b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7399ab",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5669f853",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You saw the differences between imperative, functional and array oriented programming in Jim's lecture.\n",
    "\n",
    "JAX uses a functional programming paradigm.\n",
    "\n",
    "This means we can pass the output of the gradient of a function to another gradient function, and in this manner compute any n-th order derivative. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb855c4f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c78e41",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e452e01",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second order derivative\n",
      "[[ 4.526489  -7.908262 ]\n",
      " [-7.9082613 33.122337 ]]\n",
      "\n",
      "\n",
      "Third order derivative\n",
      "[[[-1.5960214 -5.1696005]\n",
      "  [-5.169601  11.793512 ]]\n",
      "\n",
      " [[-5.1696005 11.793512 ]\n",
      "  [11.793512  33.112816 ]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Second order derivative\")\n",
    "print(np.array(  jax.jacfwd(\n",
    "                 jax.jacfwd(fnc_jax, argnums=(0,1)), \n",
    "                                     argnums=(0,1))\n",
    "                 (1.0,1.0)\n",
    "              )\n",
    "     )\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Third order derivative\")\n",
    "print(np.array( jax.jacfwd(\n",
    "                jax.jacfwd(\n",
    "                jax.jacfwd(fnc_jax, argnums=(0,1)), \n",
    "                                    argnums=(0,1)), \n",
    "                                    argnums=(0,1))\n",
    "                (1.0,1.0)\n",
    "              )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be7fa14",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095d5e8e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1f8ef3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That's all for today's lecture! \n",
    "\n",
    "In the next one we will explore more of how JAX performs tracing of the function to decompose into primitive operations, and then see how JIT compilation can speed up execution of the code!\n",
    "\n",
    "Let's move to the hands-on exercise!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
