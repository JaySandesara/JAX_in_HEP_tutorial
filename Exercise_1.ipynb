{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3410239d",
   "metadata": {},
   "source": [
    "In the lecture, we saw an example of forward mode auto-differentiation. \n",
    "\n",
    "Another way to compute derivatives using autodiff is the so-called backward mode.\n",
    "\n",
    "In the backward mode, there are two passes - forward and backward.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "<div>\n",
    "    <center>\n",
    "        <img src=\"figures/reverse_mode_autodiff.png\" width=\"1000\"/>\n",
    "    </center>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Notice how, in just one forward+backward pass, we compute the derivative with respect to `x1` and `x2`.\n",
    "\n",
    "This is contrary to forward mode, where we need one pass per differentiation wrt `x1` and another pass wrt `x2`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bea1a69",
   "metadata": {},
   "source": [
    "$\\textbf{Exercise}$:\n",
    "\n",
    "Re-write the differentiation function `fn_prime(x1,x2)` for the funtion `fn(x1, x2)` defined below using reverse-mode differentiation concepts. \n",
    "\n",
    "Do this by calculating the adjoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "14d53fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# This is the same function as the lecture\n",
    "\n",
    "def fn(x1, x2):\n",
    "    \n",
    "    a = x1/x2      \n",
    "    b = jnp.exp(x2)     \n",
    "    c = jnp.sin(a)\n",
    "    d = a - b\n",
    "    e = c + d\n",
    "    g = d * e\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678661c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write down the function\n",
    "\n",
    "def fn_prime(x1, x2):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131f310b",
   "metadata": {},
   "source": [
    "Once you write this down, compare the outputs with the `jax.grad` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82ba50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Your output\")\n",
    "print(fn_prime(1.0,1.0))\n",
    "\n",
    "print(\"JAX output\")\n",
    "print(jax.grad(fn, argnums=(0,1))(1.0,1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f713b732",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6afb29",
   "metadata": {},
   "source": [
    "A common step in doing a statistical analysis is calculating the Hessian matrix for a negative log-likelihood function.\n",
    "\n",
    "The Hessian matrix of a function, as you might be aware, is a matrix of second derivatives of the function wrt it's parameters:\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "<div>\n",
    "    <center>\n",
    "        <img src=\"figures/hessian.png\" width=\"500\"/>\n",
    "    </center>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "The dimensions of a Hessian matrix is thus equal to the number of parameters of the function. \n",
    "\n",
    "The more parameters we have, the larger it is. In ML models, these parameters can range from thousands to billions!\n",
    "\n",
    "This is why in most ML optimizations, a Hessian matrix is not computed by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5665e80b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de786c0d",
   "metadata": {},
   "source": [
    "But we are not intereseted in ML applications in these set of lectures. \n",
    "\n",
    "Hessian matrix is also computed for parameter fitting in the profile likelihood fit step of a typical statistical analysis. \n",
    "\n",
    "This Hessian matrix is typically $O(100)\\times O(100)$\n",
    "\n",
    "We will explore this in the next lecture, but for now let us try and calculate Hessian matrices for arbitrary functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363787c9",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf5b132",
   "metadata": {},
   "source": [
    "Let's say we have the following two (toy) function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d551f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a toy function - we will be working with passing arrays of arbitrary sizes\n",
    "\n",
    "def fn(tuple_arr):\n",
    "    \n",
    "    return jnp.sum(tuple_arr ** 2 - tuple_arr ** 3 - tuple_arr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372df428",
   "metadata": {},
   "source": [
    "Using the `jax.hessian` method, try calculating the Hessian matrix of this function at the point $(x,y) = (1,0)$\n",
    "\n",
    "Documentation: https://jax.readthedocs.io/en/latest/_autosummary/jax.hessian.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ab35b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.  0.]\n",
      " [ 0.  2.]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Hessian using the jax.hessian method\n",
    "\n",
    "from jax import hessian\n",
    "\n",
    "hess_mat = hessian(fn)(jnp.array([1.0,0.0]))\n",
    "\n",
    "print(hess_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1d664d",
   "metadata": {},
   "source": [
    "Now compute the hessian for a 100-dimensional array of randomly assigned numberse between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e21a82e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.  0.  0. ...  0.  0.  0.]\n",
      " [ 0. -4.  0. ...  0.  0.  0.]\n",
      " [ 0.  0. -4. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 0.  0.  0. ... -4.  0.  0.]\n",
      " [ 0.  0.  0. ...  0. -4.  0.]\n",
      " [ 0.  0.  0. ...  0.  0. -4.]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Hessian using the jax.hessian method\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "hess_mat = hessian(fn)(np.random.uniform(1.0,1.0,size=100))\n",
    "\n",
    "print(hess_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184cf1fe",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b687c1",
   "metadata": {},
   "source": [
    "Well that was easy, right? JAX is a powerful and easy to use tool. But we have just scratched the surface of it's capabilities.\n",
    "\n",
    "Now let's explore a bit how the Hessian is calculated under the hood (https://jax.readthedocs.io/en/latest/_modules/jax/_src/api.html#hessian):\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "def hessian(fun: Callable, argnums: Union[int, Sequence[int]] = 0,\n",
    "            has_aux: bool = False, holomorphic: bool = False) -> Callable:\n",
    "            \n",
    "            \n",
    "    return jacfwd(jacrev(fun, argnums, has_aux=has_aux, holomorphic=holomorphic),\n",
    "                argnums, has_aux=has_aux, holomorphic=holomorphic)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ccf08e",
   "metadata": {},
   "source": [
    "It seems that it is calculating the full Hessian matrix by first performing a reverse mode differentiation, and then performing a forward mode differentiation ont he output.\n",
    "\n",
    "For the scalar valued function we use, this looks like\n",
    "\n",
    "$$\\nabla f : \\mathbb{R}^{100} \\rightarrow \\mathbb{R}^{100}  \\\\ \\text{(Calculated using Reverse Mode)}$$ \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\nabla(\\nabla f) : \\mathbb{R}^{100} \\rightarrow \\mathbb{R}^{100 \\times 100} \\\\ \\text{(Calculated using Forward Mode)}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66fd44c",
   "metadata": {},
   "source": [
    "Is this the most efficient way of computation? Why not use jacfwd for both steps? Or reverse mode? Or forward mode followed by reverse mode?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261b8470",
   "metadata": {},
   "source": [
    "Let's try it out. Let's calculate the computation times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68dce3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.2 ms ± 213 µs per loop (mean ± std. dev. of 10 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r10 -n10\n",
    "\n",
    "#Below write down the simple hessian function calculated for f(1.0,1.0)\n",
    "\n",
    "jax.hessian(fn)(jnp.array(np.ones(200))).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6802434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.8 ms ± 3.3 ms per loop (mean ± std. dev. of 10 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r10 -n10\n",
    "\n",
    "#Below compute the hessian using a combination of forward mode differentiations only \n",
    "\n",
    "jax.jacfwd(jax.jacfwd(fn))(jnp.array(np.ones(200))).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47a7ef15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.5 ms ± 6 ms per loop (mean ± std. dev. of 10 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r10 -n10\n",
    "\n",
    "#Below compute the hessian using a combination of reverse mode differentiations only \n",
    "\n",
    "jax.jacrev(jax.jacrev(fn))(jnp.array(np.ones(200))).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c40e7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 ms ± 6.26 ms per loop (mean ± std. dev. of 10 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r10 -n10\n",
    "\n",
    "#Below compute the hessian after reversing the order of forward and backward modes in the jax hessian method\n",
    "\n",
    "jax.jacrev(jax.jacfwd(fn))(jnp.array(np.ones(200))).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85393bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.6 ms ± 481 µs per loop (mean ± std. dev. of 10 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r10 -n10\n",
    "\n",
    "#Below compute the hessian after reversing the order of forward and backward modes in the jax hessian method\n",
    "\n",
    "jax.jacfwd(jax.jacrev(fn))(jnp.array(np.ones(200))).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957e93ea",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44393a90",
   "metadata": {},
   "source": [
    "Can you guess a pattern in these results?\n",
    "\n",
    "Think about it carefully, based on the calculation flow used by forward and backward modes of automatic differentiation. Which do you think would be optimal? And when?\n",
    "\n",
    "I would recommend you discuss with your colleagues and come up with a satisfactory answer before moving down!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d077126",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35804f0e",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74e1ed3",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ccfdf1",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88220fac",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995ce6ca",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bd9d27",
   "metadata": {},
   "source": [
    "Remember how reverse-mode autodiff works! \n",
    "\n",
    "In just a single pass, we calculate the derivatives wrt all the input features. i.e. we build the Jacobian one row at a time with reverse mode differentiation.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "<div>\n",
    "    <center>\n",
    "        <img src=\"figures/Jacobian.webp\" width=\"500\"/>\n",
    "    </center>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "So for scalar valued functions $f:\\mathbb{R}^n \\rightarrow \\mathbb{R}$ like the one we have reverse-mode autodiff works best to compute the Jacbian $\\nabla f:\\mathbb{R}^n \\rightarrow \\mathbb{R}^n$.\n",
    "\n",
    "On the other hand, with forward mode automatic differentiation, we calculate the Jacobian one column at a time. In just a single forward pass we calculate the derivatives of a function with arbitrary output dimension.\n",
    "\n",
    "As a general rule of thumb, for differentation of functions of the type $f:\\mathbb{R}^n \\rightarrow \\mathbb{R}^m$: \n",
    "\n",
    "if $m \\ll n$, use reverse mode autodiff, and if $m \\gg n$ we use forward mode autodiff. \n",
    "\n",
    "Let's verify this once more:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8725bf",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc79e9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.32 ms ± 188 µs per loop (mean ± std. dev. of 30 runs, 30 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r30 -n30\n",
    "\n",
    "jax.jacrev(fn)(jnp.array(np.ones(1000))).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33adf6b6",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0049a1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7 ms ± 315 µs per loop (mean ± std. dev. of 30 runs, 30 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r30 -n30\n",
    "\n",
    "jax.jacfwd(fn)(jnp.array(np.ones(1000))).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f06638",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a582d275",
   "metadata": {},
   "source": [
    "For simple functions, though, the time difference is not very significant.\n",
    "\n",
    "That is it for today's exercise! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a9e67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
